<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>ReAct: Synergizing Reasoning and Acting in Language Models</title><meta name="description" content="Research website of Zhuoyang Zou, PhD student in Computer Science specializing in LLMs and multi-agent systems"/><link rel="icon" href="/favicon.ico"/><meta name="next-head-count" content="5"/><link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet"/><style>
          body {
            background-color: rgb(245, 247, 250);
            font-family: -apple-system, BlinkMacSystemFont, &#x27;Segoe UI&#x27;, Roboto, Oxygen,
              Ubuntu, Cantarell, &#x27;Open Sans&#x27;, &#x27;Helvetica Neue&#x27;, sans-serif;
          }
          h1, h2, h3, h4, h5, h6 {
            font-family: &#x27;Georgia&#x27;, serif;
          }
          .bg-blue-900 {
            background-color: #192b6a;
          }
          .bg-blue-50 {
            background-color: #f0f5ff;
          }
          .text-blue-900 {
            color: #192b6a;
          }
          .border-blue-900 {
            border-color: #192b6a;
          }
          .hover\:text-blue-200:hover {
            color: #b4c6fc;
          }
          .border-blue-700 {
            border-color: #1a56db;
          }
          .border-green-700 {
            border-color: #046c4e;
          }
          .border-indigo-700 {
            border-color: #4338ca;
          }
        </style><link rel="preload" href="/zhuoyang/_next/static/css/aa3c483bfeaf88c7.css" as="style" crossorigin=""/><link rel="stylesheet" href="/zhuoyang/_next/static/css/aa3c483bfeaf88c7.css" crossorigin="" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" crossorigin="" nomodule="" src="/zhuoyang/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/zhuoyang/_next/static/chunks/webpack-c9d895807f7750df.js" defer="" crossorigin=""></script><script src="/zhuoyang/_next/static/chunks/framework-6d18543c2c368b0c.js" defer="" crossorigin=""></script><script src="/zhuoyang/_next/static/chunks/main-b3ca0e543ab69423.js" defer="" crossorigin=""></script><script src="/zhuoyang/_next/static/chunks/pages/_app-3b9b0d5f6bc20c99.js" defer="" crossorigin=""></script><script src="/zhuoyang/_next/static/chunks/1bfc9850-a0833b6140f33d57.js" defer="" crossorigin=""></script><script src="/zhuoyang/_next/static/chunks/636-697f7ca980ebb593.js" defer="" crossorigin=""></script><script src="/zhuoyang/_next/static/chunks/pages/blog/1-17e70dc59357ab0a.js" defer="" crossorigin=""></script><script src="/zhuoyang/_next/static/8MFiiFwKkqKNqED7qBe7r/_buildManifest.js" defer="" crossorigin=""></script><script src="/zhuoyang/_next/static/8MFiiFwKkqKNqED7qBe7r/_ssgManifest.js" defer="" crossorigin=""></script></head><body><div id="__next"><div class="flex flex-col min-h-screen"><header class="bg-blue-900 text-white shadow-md"><div class="container mx-auto px-4 py-5"><div class="flex flex-col md:flex-row md:justify-between md:items-center"><div class="text-2xl font-bold mb-4 md:mb-0"><a class="hover:text-blue-200 transition duration-300" href="/zhuoyang/">Zhuoyang Zou</a></div><nav><ul class="flex space-x-6"><li><a class="hover:text-blue-200 transition duration-300 " href="/zhuoyang/">Home</a></li><li><a class="hover:text-blue-200 transition duration-300 " href="/zhuoyang/research/">Research</a></li><li><a class="hover:text-blue-200 transition duration-300 " href="/zhuoyang/blog/">Blog</a></li></ul></nav></div></div></header><main class="flex-grow container mx-auto px-4 py-8"><article class="max-w-4xl mx-auto bg-white rounded-lg shadow-md p-8"><div class="mb-8"><a href="/zhuoyang/blog/" class="text-blue-600 hover:underline flex items-center">← Back to all posts</a></div><h1 class="text-3xl font-bold text-blue-900 mb-4">ReAct: When AI Finally Learns to Think AND Do</h1><div class="text-gray-600 mb-6">May 16, 2025</div><div class="flex flex-wrap gap-2 mb-8"><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm">LLM</span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm">Multi-agent systems</span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm">Reasoning</span></div><div class="prose prose-lg max-w-none"><h2 class="text-2xl font-semibold text-blue-800 mt-6 mb-4">Remember When AI Was Just a Fancy Autocomplete?</h2><p>Early LLMs were basically just glorified know-it-alls. You&#x27;d ask them something, and they&#x27;d spit out whatever sounded right based on their training data. It was impressive, sure, but these models had all the situational awareness of a goldfish – they&#x27;d give you one answer and call it a day, with zero ability to check if they were even making sense.</p><div class="bg-yellow-50 border-l-4 border-yellow-500 p-4 my-6"><p class="italic">&quot;Early language models were like that friend who confidently gives you directions but has never actually been to the place themselves.&quot;</p></div><h2 class="text-2xl font-semibold text-blue-800 mt-6 mb-4">So What&#x27;s This ReAct Thing All About?</h2><p>ReAct is basically what happens when AI learns to walk and chew gum at the same time. The name is just &quot;Reasoning + Acting&quot; smooshed together, which pretty much tells you the whole story. It was cooked up by Yao and friends in their paper <a href="https://arxiv.org/abs/2210.03629" class="text-blue-600 hover:underline">ReAct: Synergizing Reasoning and Acting in Language Models</a>, and it&#x27;s all about getting language models to think and do stuff in a continuous loop.</p><h2 class="text-2xl font-semibold text-blue-800 mt-6 mb-4">The Magic Loop: Think, Do, Look, Repeat</h2><p>Here&#x27;s where things get cool. Instead of just thinking in a vacuum, ReAct models follow this cycle:</p><div class="bg-blue-50 p-5 rounded-lg my-6"><div class="flex flex-col md:flex-row gap-4"><div class="bg-white p-4 rounded shadow-sm flex-1"><h4 class="font-bold text-lg mb-2">Think</h4><p>The AI considers what&#x27;s happening and what to do next</p></div><div class="bg-white p-4 rounded shadow-sm flex-1"><h4 class="font-bold text-lg mb-2">Act</h4><p>It actually DOES something (like searching Google)</p></div><div class="bg-white p-4 rounded shadow-sm flex-1"><h4 class="font-bold text-lg mb-2">Observe</h4><p>It sees what happened as a result</p></div><div class="bg-white p-4 rounded shadow-sm flex-1"><h4 class="font-bold text-lg mb-2">Repeat</h4><p>Back to thinking with new info!</p></div></div></div><p>It&#x27;s like the difference between trying to navigate a city using only an outdated map versus actually walking around, looking at street signs, and asking for directions when you get lost. One approach is bound to get you into trouble; the other is way more reliable.</p><h2 class="text-2xl font-semibold text-blue-800 mt-6 mb-4">ReAct vs. Chain-of-Thought: An Everyday Example</h2><p>Let me break this down with something we all do – cooking. Here&#x27;s how these two approaches would handle making breakfast:</p><div class="bg-gray-50 p-4 rounded-md my-4"><h3 class="font-medium text-lg mb-2">Chain-of-Thought (The Overconfident Chef):</h3><pre class="bg-gray-100 p-3 rounded text-sm overflow-x-auto">1. Thought: I need to boil eggs<br/>2. Thought: Go to the refrigerator<br/>3. Thought: Take out the eggs<br/>4. Thought: Boil water in a pot<br/>5. Thought: Place eggs in the boiling water<br/>6. Thought: Wait for five minutes<br/>7. Thought: Remove eggs and cool them</pre></div><p>This is like planning your entire cooking process without ever opening the fridge. What if you don&#x27;t have eggs? What if your roommate used the last ones for that midnight omelet? Chain-of-Thought just assumes everything will go according to plan.</p><div class="bg-gray-50 p-4 rounded-md my-4"><h3 class="font-medium text-lg mb-2">ReAct (The Practical Chef):</h3><pre class="bg-gray-100 p-3 rounded text-sm overflow-x-auto">1. Thought: I want to make boiled eggs for breakfast. Let me check if we have eggs.<br/>2. Action: *Opens refrigerator*<br/>3. Observation: Whoa, no eggs in here. Just ketchup and suspicious leftovers.<br/>4. Thought: Need to check the pantry, sometimes we store eggs there.<br/>5. Action: *Looks in pantry*<br/>6. Observation: Found a carton with two eggs! They&#x27;re not refrigerated but seem fine.<br/>7. Thought: Great! Now I need to boil some water.<br/>8. Action: *Fills pot with water and puts it on stove*<br/>...</pre></div><p>See the difference? ReAct actually checks if you have eggs before committing to the whole plan. It adapts based on what it finds in the real world. That&#x27;s the game-changer here!</p><div class="bg-green-50 border-l-4 border-green-500 p-4 my-6"><p><strong>ReAct in a nutshell:</strong> Don&#x27;t just think about stuff – check if it actually works, then adjust your plan based on reality!</p></div><h2 class="text-2xl font-semibold text-blue-800 mt-6 mb-4">Why This is Actually a Big Deal</h2><p>ReAct really shines when you need AI to:</p><ul class="list-disc pl-6 my-4 space-y-2"><li><strong>Fact-check itself</strong> - &quot;Wait, let me Google that before I say something stupid&quot;</li><li><strong>Solve tricky problems</strong> - Breaking things down and checking each step actually works</li><li><strong>Have actual conversations</strong> - Responding to what YOU say, not what it thinks you might have said</li><li><strong>Use tools</strong> - Like grabbing a calculator when the math gets hairy</li></ul><h2 class="text-2xl font-semibold text-blue-800 mt-6 mb-4">How to Make It Work (The Techy Bit)</h2><p>If you want to implement ReAct (and honestly, why wouldn&#x27;t you?), you need three main ingredients:</p><ol class="list-decimal pl-6 my-4 space-y-2"><li>A clear <strong>task description</strong> (like &quot;Find me the best pizza place in Boston&quot;)</li><li>Some <strong>examples</strong> showing the AI how to alternate between Thought, Action, and Observation</li><li>A <strong>prompt structure</strong> that keeps the AI on track with this pattern</li></ol><p>You&#x27;ll also need to decide what actions your AI can take (search the web? use a calculator? check a database?), create ways for those actions to actually work, and design prompts that encourage your AI to think out loud and pick smart actions.</p><p>It&#x27;s not rocket science, but it does take some work to set up right. The payoff is worth it though!</p><h2 class="text-2xl font-semibold text-blue-800 mt-6 mb-4">Why I&#x27;m Excited About ReAct</h2><p>This isn&#x27;t just a minor upgrade – it&#x27;s a fundamental shift in how AI works. We&#x27;re moving from &quot;AI that knows stuff&quot; to &quot;AI that figures stuff out.&quot; Instead of just being walking encyclopedias, these models can now be problem-solvers that interact with the world.</p><p>The idea seems simple (duh, check if your assumptions are correct), but the impact is huge. With ReAct, AI can:</p><div class="grid grid-cols-1 md:grid-cols-2 gap-4 my-6"><div class="bg-blue-50 p-4 rounded"><p class="font-semibold">Actually verify facts</p><p class="text-sm">Instead of making stuff up, it can check sources</p></div><div class="bg-blue-50 p-4 rounded"><p class="font-semibold">Handle complex problems</p><p class="text-sm">By breaking them down and checking each step works</p></div><div class="bg-blue-50 p-4 rounded"><p class="font-semibold">Build on solid ground</p><p class="text-sm">Using real information instead of assumptions</p></div><div class="bg-blue-50 p-4 rounded"><p class="font-semibold">Create feedback loops</p><p class="text-sm">Between thinking and real-world info</p></div></div><p>The coolest part? ReAct sets the stage for AI that can pursue goals on its own while still showing its work, so we humans can follow along and understand what it&#x27;s doing.</p><p>Of course, it&#x27;s not perfect. ReAct only works as well as the actions it can take and how good it is at interpreting what it observes. But as these pieces improve, watch out – these systems are getting smarter fast!</p><h2 class="text-2xl font-semibold text-blue-800 mt-6 mb-4">The Bottom Line</h2><p>ReAct is a game-changer because it gives AI a way to check itself against reality. Instead of living entirely in its training data bubble, it can reach out, interact with the world, and adjust course when needed.</p><p>It&#x27;s like the difference between someone who insists they know how to get to the restaurant because they memorized a map once versus someone who pulls out Google Maps and checks for traffic in real-time. Which person would you rather ride with?</p><div class="bg-purple-50 p-5 rounded-lg my-6"><h3 class="text-xl font-medium text-purple-700 mb-3">Tomorrow&#x27;s AI Today</h3><p class="text-lg">By combining thinking and doing, ReAct represents a huge leap toward AI assistants that can actually navigate the messiness of the real world. And in my book, that&#x27;s pretty darn exciting.</p></div></div></article></main><footer class="bg-gray-100 text-gray-700 py-8"><div class="container mx-auto px-4"><div class="flex flex-col md:flex-row justify-between items-center"><div class="mb-4 md:mb-0"><p>© <!-- -->2026<!-- --> Zhuoyang Zou. All rights reserved.</p></div><div class="flex space-x-6"><a href="mailto:mintzou2000@gmail.com" class="hover:text-blue-800 transition duration-300"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="text-xl" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://github.com/zyzou8" target="_blank" rel="noopener noreferrer" class="hover:text-blue-800 transition duration-300"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" class="text-xl" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://www.linkedin.com/in/zhuoyang-zou-44b2b3238/" target="_blank" rel="noopener noreferrer" class="hover:text-blue-800 transition duration-300"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="text-xl" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json" crossorigin="">{"props":{"pageProps":{}},"page":"/blog/1","query":{},"buildId":"8MFiiFwKkqKNqED7qBe7r","assetPrefix":"/zhuoyang","nextExport":true,"autoExport":true,"isFallback":false,"scriptLoader":[]}</script></body></html>