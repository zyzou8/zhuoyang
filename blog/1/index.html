<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Evaluating LLM Multi-Agent Systems - Zhuoyang Zou&#x27;s Blog</title><meta name="description" content="Research website of Zhuoyang Zou, PhD student in Computer Science specializing in LLMs and multi-agent systems"/><link rel="icon" href="/favicon.ico"/><meta name="next-head-count" content="5"/><link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet"/><style>
          body {
            background-color: rgb(245, 247, 250);
            font-family: -apple-system, BlinkMacSystemFont, &#x27;Segoe UI&#x27;, Roboto, Oxygen,
              Ubuntu, Cantarell, &#x27;Open Sans&#x27;, &#x27;Helvetica Neue&#x27;, sans-serif;
          }
          h1, h2, h3, h4, h5, h6 {
            font-family: &#x27;Georgia&#x27;, serif;
          }
          .bg-blue-900 {
            background-color: #192b6a;
          }
          .bg-blue-50 {
            background-color: #f0f5ff;
          }
          .text-blue-900 {
            color: #192b6a;
          }
          .border-blue-900 {
            border-color: #192b6a;
          }
          .hover\:text-blue-200:hover {
            color: #b4c6fc;
          }
          .border-blue-700 {
            border-color: #1a56db;
          }
          .border-green-700 {
            border-color: #046c4e;
          }
          .border-indigo-700 {
            border-color: #4338ca;
          }
        </style><link rel="preload" href="/zhuoyang/_next/static/css/af50d1c3f107e91d.css" as="style" crossorigin=""/><link rel="stylesheet" href="/zhuoyang/_next/static/css/af50d1c3f107e91d.css" crossorigin="" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" crossorigin="" nomodule="" src="/zhuoyang/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/zhuoyang/_next/static/chunks/webpack-c9d895807f7750df.js" defer="" crossorigin=""></script><script src="/zhuoyang/_next/static/chunks/framework-6d18543c2c368b0c.js" defer="" crossorigin=""></script><script src="/zhuoyang/_next/static/chunks/main-b3ca0e543ab69423.js" defer="" crossorigin=""></script><script src="/zhuoyang/_next/static/chunks/pages/_app-3b9b0d5f6bc20c99.js" defer="" crossorigin=""></script><script src="/zhuoyang/_next/static/chunks/1bfc9850-ca7360a6272e6ade.js" defer="" crossorigin=""></script><script src="/zhuoyang/_next/static/chunks/636-697f7ca980ebb593.js" defer="" crossorigin=""></script><script src="/zhuoyang/_next/static/chunks/pages/blog/1-d8d6a3e20a606e03.js" defer="" crossorigin=""></script><script src="/zhuoyang/_next/static/ouqjq_7FgCxh7GwHN7oYi/_buildManifest.js" defer="" crossorigin=""></script><script src="/zhuoyang/_next/static/ouqjq_7FgCxh7GwHN7oYi/_ssgManifest.js" defer="" crossorigin=""></script></head><body><div id="__next"><div class="flex flex-col min-h-screen"><header class="bg-blue-900 text-white shadow-md"><div class="container mx-auto px-4 py-5"><div class="flex flex-col md:flex-row md:justify-between md:items-center"><div class="text-2xl font-bold mb-4 md:mb-0"><a class="hover:text-blue-200 transition duration-300" href="/zhuoyang/">Zhuoyang Zou</a></div><nav><ul class="flex space-x-6"><li><a class="hover:text-blue-200 transition duration-300 " href="/zhuoyang/">Home</a></li><li><a class="hover:text-blue-200 transition duration-300 " href="/zhuoyang/research/">Research</a></li><li><a class="hover:text-blue-200 transition duration-300 " href="/zhuoyang/blog/">Blog</a></li></ul></nav></div></div></header><main class="flex-grow container mx-auto px-4 py-8"><article class="max-w-4xl mx-auto bg-white rounded-lg shadow-md p-8"><div class="mb-8"><a href="/zhuoyang/blog/" class="text-blue-600 hover:underline flex items-center">← Back to all posts</a></div><h1 class="text-3xl font-bold text-blue-900 mb-4">ReAct: Synergizing Reasoning and Acting in Language Models</h1><div class="text-gray-600 mb-6">May 16, 2025</div><div class="flex flex-wrap gap-2 mb-8"><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm">LLM</span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm">Multi-agent systems</span><span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm">Research</span></div><div class="prose prose-lg max-w-none"><h2 class="text-2xl font-semibold text-blue-800 mt-6 mb-4">Introduction: The Evolution of LLM Capabilities</h2><p>Language Models have evolved dramatically over the past few years. Initially, LLMs were primarily designed to process user queries and generate relevant responses based on their training data—essentially functioning as sophisticated knowledge repositories with limited reasoning capabilities. When prompted, these early models would typically produce a single-step response without much deliberation or interaction with external information.</p><h2 class="text-2xl font-semibold text-blue-800 mt-6 mb-4">What is ReAct?</h2><p>ReAct, short for &quot;Reasoning + Acting,&quot; represents a significant advancement in how language models approach complex tasks. Introduced in the paper <a href="https://arxiv.org/abs/2210.03629" class="text-blue-600 hover:underline">ReAct: Synergizing Reasoning and Acting in Language Models</a> by Yao et al., this framework enables LLMs to interleave reasoning traces and task-specific actions in a synergistic way.</p><h2 class="text-2xl font-semibold text-blue-800 mt-6 mb-4">How ReAct Works: The Reasoning-Acting Loop</h2><p>The core innovation of ReAct lies in its cyclical process:</p><ol class="list-decimal pl-6 my-4"><li class="mb-2"><strong>Reasoning</strong>: The model first reasons about the current state and decides what actions to take</li><li class="mb-2"><strong>Acting</strong>: It executes the chosen action (like searching for information)</li><li class="mb-2"><strong>Observing</strong>: It obtains feedback from the environment based on the action</li><li class="mb-2"><strong>Reasoning Again</strong>: It processes this new information to plan the next steps</li></ol><p>This creates a dynamic loop where the model continuously refines its understanding through interaction with external resources and tools.</p><h2 class="text-2xl font-semibold text-blue-800 mt-6 mb-4">ReAct vs. Chain-of-Thought (CoT): A Practical Comparison</h2><p>To understand the difference between ReAct and Chain-of-Thought (CoT), let&#x27;s compare how each would approach a simple cooking task:</p><div class="bg-gray-50 p-4 rounded-md my-4"><h3 class="font-medium text-lg mb-2">Chain-of-Thought (CoT) Approach:</h3><pre class="bg-gray-100 p-3 rounded text-sm overflow-x-auto">1. Thought: I need to boil eggs<br/>2. Thought: Go to the refrigerator<br/>3. Thought: Take out the eggs<br/>4. Thought: Boil water in a pot<br/>5. Thought: Place eggs in the boiling water<br/>6. Thought: Wait for five minutes<br/>7. Thought: Remove eggs and cool them</pre></div><p>In CoT, the model outlines a complete thought process but operates under assumptions that might not match reality. It has no mechanism to verify if eggs are actually available in the refrigerator.</p><div class="bg-gray-50 p-4 rounded-md my-4"><h3 class="font-medium text-lg mb-2">ReAct Approach:</h3><pre class="bg-gray-100 p-3 rounded text-sm overflow-x-auto">1. Thought: I need to boil eggs. First, I need to check if we have eggs.<br/>2. Action: Check refrigerator for eggs<br/>3. Observation: There are no eggs in the refrigerator<br/>4. Thought: I need to get eggs before I can boil them<br/>5. Action: Check pantry for eggs<br/>6. Observation: Found a carton with two eggs in the pantry<br/>7. Thought: Now I need to boil water for the eggs<br/>8. Action: Fill pot with water and place on stove<br/>...</pre></div><p>ReAct constantly grounds its reasoning in reality by taking actions and making observations, allowing it to adapt to the actual state of the environment rather than proceeding based on assumptions.</p><h2 class="text-2xl font-semibold text-blue-800 mt-6 mb-4">Why ReAct Matters: Practical Applications</h2><p>ReAct&#x27;s approach has proven particularly effective for tasks requiring:</p><ul class="list-disc pl-6 my-4"><li class="mb-2"><strong>Fact verification</strong>: The model can search for and verify information rather than relying solely on its training data</li><li class="mb-2"><strong>Complex problem-solving</strong>: Breaking down problems and checking intermediate steps</li><li class="mb-2"><strong>Interactive tasks</strong>: Customer service, tutoring, or any scenario requiring back-and-forth exchanges</li><li class="mb-2"><strong>Tool use</strong>: Leveraging external tools like calculators, search engines, or databases</li></ul><h2 class="text-2xl font-semibold text-blue-800 mt-6 mb-4">Technical Implementation</h2><p>From a technical perspective, ReAct prompts typically include:</p><ol class="list-decimal pl-6 my-4"><li class="mb-2">A <strong>task description</strong></li><li class="mb-2"><strong>Few-shot examples</strong> demonstrating the interleaving of Thought, Action, and Observation</li><li class="mb-2">A <strong>prompt structure</strong> that encourages the model to follow the ReAct pattern</li></ol><p>The implementation requires:</p><ul class="list-disc pl-6 my-4"><li class="mb-2">Defining available actions (search, calculate, query database, etc.)</li><li class="mb-2">Creating mechanisms to execute these actions and return observations</li><li class="mb-2">Designing prompts that encourage the model to reason explicitly and choose appropriate actions</li></ul><h2 class="text-2xl font-semibold text-blue-800 mt-6 mb-4">Reflections: The Significance of ReAct</h2><p>ReAct represents a fundamental shift in how we conceptualize AI assistants. By combining deliberate reasoning with the ability to interact with the environment, these systems move from being passive knowledge repositories to active problem-solvers.</p><p>While the idea may seem straightforward, its implications are profound. ReAct enables LLMs to:</p><ol class="list-decimal pl-6 my-4"><li class="mb-2"><strong>Overcome hallucination issues</strong> by grounding responses in external verified information</li><li class="mb-2"><strong>Tackle more complex tasks</strong> by breaking them down and verifying each step</li><li class="mb-2"><strong>Build on factual foundations</strong> rather than assumptions</li><li class="mb-2"><strong>Create a true feedback loop</strong> between reasoning and real-world information</li></ol><p>Perhaps most importantly, ReAct lays the groundwork for more agentic AI systems that can autonomously pursue goals while maintaining a clear reasoning trace that humans can follow and understand.</p><p>The limitations of ReAct lie primarily in its dependence on the quality of available actions and the model&#x27;s ability to interpret observations correctly. As these components improve, we can expect ReAct-based systems to handle increasingly sophisticated tasks.</p><h2 class="text-2xl font-semibold text-blue-800 mt-6 mb-4">Conclusion</h2><p>ReAct represents a crucial step toward more capable, grounded, and transparent AI assistants. By bridging the gap between reasoning and acting, it enables language models to engage with the world more effectively and provide more reliable assistance across a wide range of applications.</p><p>As we continue to develop these systems, maintaining this synergy between thought and action will be essential for creating AI that can truly understand and assist with the complexity of real-world tasks.</p></div></article></main><footer class="bg-gray-100 text-gray-700 py-8"><div class="container mx-auto px-4"><div class="flex flex-col md:flex-row justify-between items-center"><div class="mb-4 md:mb-0"><p>© <!-- -->2025<!-- --> Zhuoyang Zou. All rights reserved.</p></div><div class="flex space-x-6"><a href="mailto:mintzou2000@gmail.com" class="hover:text-blue-800 transition duration-300"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="text-xl" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg></a><a href="https://github.com/zyzou8" target="_blank" rel="noopener noreferrer" class="hover:text-blue-800 transition duration-300"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" class="text-xl" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a href="https://www.linkedin.com/in/zhuoyang-zou-44b2b3238/" target="_blank" rel="noopener noreferrer" class="hover:text-blue-800 transition duration-300"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" class="text-xl" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></div></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json" crossorigin="">{"props":{"pageProps":{}},"page":"/blog/1","query":{},"buildId":"ouqjq_7FgCxh7GwHN7oYi","assetPrefix":"/zhuoyang","nextExport":true,"autoExport":true,"isFallback":false,"scriptLoader":[]}</script></body></html>